{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgwZtzCyvTYf"
   },
   "source": [
    "# Семинар 9. Работа с текстами\n",
    "\n",
    "![Картинка](https://res.cloudinary.com/practicaldev/image/fetch/s--pRQGS79D--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/c2eanc5wieohaa3scthc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbHlWheTvTYi"
   },
   "source": [
    "### Примеры задач автоматической обработки текстов:\n",
    "\n",
    "- классификация текстов\n",
    "\n",
    "    - анализ тональности (например, позитивный/негативный отзыв)\n",
    "    - фильтрация спама\n",
    "    - по теме или жанру\n",
    "\n",
    "- машинный перевод\n",
    "\n",
    "- распознавание речи\n",
    "\n",
    "- извлечение информации\n",
    "\n",
    "    - именованные сущности (например, извлечение имен, локаций, названий организаций)\n",
    "    - извлечение фактов и событий\n",
    "\n",
    "- кластеризация текстов\n",
    "\n",
    "- оптическое распознавание символов\n",
    "\n",
    "- проверка правописания\n",
    "\n",
    "- вопросно-ответные системы\n",
    "\n",
    "- суммаризация текстов\n",
    "\n",
    "- генерация текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krXjGbjhvTYj"
   },
   "source": [
    "### Классические методы для работы с текстами:\n",
    "\n",
    "- токенизация\n",
    "\n",
    "- лемматизация / стемминг\n",
    "\n",
    "- удаление стоп-слов и пунктуации\n",
    "\n",
    "- векторное представление текстов (bag of words и TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTYWd8k5vTYk"
   },
   "source": [
    "## Токенизация\n",
    "\n",
    "Токенизировать — значит, поделить текст на слова, или *токены*.\n",
    "\n",
    "Самый наивный способ токенизировать текст — разделить с помощью `split`. Но `split` упускает очень много всего, например, не отделяет пунктуацию от слов. Кроме этого, есть ещё много менее тривиальных проблем. Поэтому лучше использовать готовые токенизаторы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7wYTyUXsvTYk",
    "outputId": "55a6eabb-3b53-4502-d9b3-437f1847434a"
   },
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0hCRd91OvTYl",
    "outputId": "9f180778-098e-4a9d-a3cc-aa3cdf905f6d"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BnwtbxVSvTYl"
   },
   "outputs": [],
   "source": [
    "example = 'Но не каждый хочет что-то исправлять:('"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p2GQ7QsjvTYm",
    "outputId": "784207cc-6ba2-4e96-8066-d733c7786685"
   },
   "outputs": [],
   "source": [
    "# c помощью split()\n",
    "example.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nA0DyFIDvTYn",
    "outputId": "df8fdb70-4d7a-476b-9496-78562e7e5912"
   },
   "outputs": [],
   "source": [
    "# c помощью токенизатора\n",
    "word_tokenize(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQ7uHEJUvTYo"
   },
   "source": [
    "В nltk вообще есть довольно много токенизаторов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_fnNryNkvTYo",
    "outputId": "0aae1ea4-65f5-465c-eda4-199c3641b232"
   },
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "dir(tokenize)[:16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cs6tdtHvTYp"
   },
   "source": [
    "Можно получить индексы начала и конца каждого токена:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3kOepXRvTYp"
   },
   "outputs": [],
   "source": [
    "wh_tok = tokenize.WhitespaceTokenizer()\n",
    "list(wh_tok.span_tokenize(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeMsc1yVvTYp"
   },
   "source": [
    "Некторые токенизаторы ведут себя специфично:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5diW1OKkvTYq",
    "outputId": "0d12988f-05ec-433c-f25d-357a1adfc61a"
   },
   "outputs": [],
   "source": [
    "tokenize.TreebankWordTokenizer().tokenize(\"don't stop me\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4Jte9_YvTYq"
   },
   "source": [
    "Для некоторых задач это может быть полезно.\n",
    "\n",
    "А некоторые предназначены вообще не для текста на естественном языке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vC4pjzzWvTYr",
    "outputId": "ee796848-f30a-45aa-b2fb-11d0d0c87a31"
   },
   "outputs": [],
   "source": [
    "tokenize.SExprTokenizer().tokenize(\"(a (b c)) d e (f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTLQY6ndwP9u"
   },
   "source": [
    "Есть токенизатор, который может быть полезен для работы с твитами или сообщениями из соц. сетей.\n",
    "Он сохранит смайлики, хештеги и т.п."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xioD_3G2vTYs",
    "outputId": "155e9e32-9d62-4223-d575-df678aca92c7"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tw = TweetTokenizer()\n",
    "tw.tokenize(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxWB7BzKvTYs"
   },
   "source": [
    "_Что почитать:_\n",
    "\n",
    "- http://mlexplained.com/2019/11/06/a-deep-dive-into-the-wonderful-world-of-preprocessing-in-nlp/\n",
    "- https://blog.floydhub.com/tokenization-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fc1OYy0tvTYt"
   },
   "source": [
    "## Стоп-слова и пунктуация\n",
    "\n",
    "*Стоп-слова* — это слова, которые часто встречаются практически в любом тексте и ничего интересного не говорят о конретном документе, то есть играют роль шума. Поэтому их принято убирать. По той же причине убирают и пунктуацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g2F7i9CEvTYt",
    "outputId": "e8d5b428-583d-46a3-ac67-7f6ed7763dfc"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r8NdjFCYvTYt",
    "outputId": "bab6dba5-a1d6-480a-8a79-998de1c6c20f"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "2DehgAP3vTYu",
    "outputId": "25f5ddde-9521-4a76-803e-126ad14b1a3b"
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLzygOQ6vTYv"
   },
   "outputs": [],
   "source": [
    "noise = stopwords.words('russian') + list(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SEdeiiPvTYv"
   },
   "source": [
    "## Лемматизация\n",
    "\n",
    "[**Лемматизация**](https://en.wikipedia.org/wiki/Lemmatisation) — процесс приведения слова к его нормальной форме (**лемме**):\n",
    "- для существительных — именительный падеж, единственное число;\n",
    "- для прилагательных — именительный падеж, единственное число, мужской род;\n",
    "- для глаголов, причастий, деепричастий — глагол в инфинитиве.\n",
    "\n",
    "\n",
    "Например, токены «пью», «пил», «пьет» перейдут в «пить». Почему это хорошо?\n",
    "* Во-первых, мы хотим рассматривать как отдельный признак каждое *слово*, а не каждую его отдельную форму.\n",
    "* Во-вторых, некоторые стоп-слова стоят только в начальной форме, и без лемматизации выкидываем мы только её.\n",
    "\n",
    "Для русского есть два хороших лемматизатора: `mystem` и `pymorphy`.\n",
    "\n",
    "### [Mystem](https://tech.yandex.ru/mystem/)\n",
    "Как с ним работать:\n",
    "* можно скачать mystem и запускать [из терминала с разными параметрами](https://tech.yandex.ru/mystem/doc/)\n",
    "* [pymystem3](https://pythonhosted.org/pymystem3/pymystem3.html) - обертка для питона, работает медленнее, но это удобно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IM5dc5MPvTYw",
    "outputId": "fa540fd2-7a6a-476f-ab32-b0fd37e86461",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60Q4xp6yvTYw"
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import pymystem3\n",
    "mystem_analyzer = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "al294srlvTYx"
   },
   "source": [
    "Мы инициализировали Mystem c дефолтными параметрами. А вообще параметры есть такие:\n",
    "* mystem_bin — путь к `mystem`, если их несколько\n",
    "* grammar_info — нужна ли грамматическая информация или только леммы (по умолчанию нужна)\n",
    "* disambiguation — нужно ли снятие [омонимии](https://ru.wikipedia.org/wiki/%D0%9E%D0%BC%D0%BE%D0%BD%D0%B8%D0%BC%D1%8B) - дизамбигуация (по умолчанию нужна)\n",
    "* entire_input — нужно ли сохранять в выводе все (пробелы, например), или можно выкинуть (по умолчанию оставляется все)\n",
    "\n",
    "Методы Mystem принимают строку, токенизатор вшит внутри. Можно, конечно, и пословно анализировать, но тогда он не сможет учитывать контекст.\n",
    "\n",
    "Можно просто лемматизировать текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Oc4plDGvTYx",
    "outputId": "dd71e6d0-a98b-4a5c-fa3c-30370a493425"
   },
   "outputs": [],
   "source": [
    "print(mystem_analyzer.lemmatize(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mX1O12xvTYy"
   },
   "source": [
    "### [Pymorphy](http://pymorphy2.readthedocs.io/en/latest/)\n",
    "Это модуль на питоне, довольно быстрый и с кучей функций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l7-NcH9ivTYy",
    "outputId": "52fc5c7e-408c-4838-c022-84c9c340349b"
   },
   "outputs": [],
   "source": [
    "#!pip install pymorphy2\n",
    "#!pip install pymorphy2-dicts\n",
    "#!pip install DAWG-Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnL4uv6JvTYz"
   },
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "pymorphy2_analyzer = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIe3ssarvTYz"
   },
   "source": [
    "pymorphy2 работает с отдельными словами. Если дать ему на вход предложение — он его просто не лемматизирует, т.к. не понимает.\n",
    "\n",
    "Метод MorphAnalyzer.parse() принимает слово  и возвращает все возможные его разборы.\n",
    "\n",
    "У каждого разбора есть тег.\n",
    "Тег - это набор граммем, характеризующих данное слово. Например, тег 'VERB,perf,intr plur,past,indc' означает, что слово - глагол (VERB) совершенного вида (perf), непереходный (intr), множественного числа (plur), прошедшего времени (past), изъявительного наклонения (indc).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Ztv5u7gvTY0",
    "outputId": "b70bae89-562b-4ce3-ba4c-e10a10469fd6"
   },
   "outputs": [],
   "source": [
    "ana = pymorphy2_analyzer.parse('хочет')\n",
    "ana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "9NBnUjhfvTY2",
    "outputId": "45f3f0ca-770e-4436-a0d0-9006562c4d86"
   },
   "outputs": [],
   "source": [
    "ana[0].normal_form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AY0CeomBvTY3"
   },
   "source": [
    "### mystem vs. pymorphy\n",
    "\n",
    "1) *Надеемся, что вы пользуетесь линуксом или маком* — mystem работает невероятно медленно под windows на больших текстах\n",
    "\n",
    "2) *Снятие омонимии*. Mystem умеет снимать омонимию по контексту (хотя не всегда преуспевает), pymorphy2 берет на вход одно слово и соответственно вообще не умеет дизамбигуировать по контексту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ZHNWM8MvTY3",
    "outputId": "516e91d4-e73b-41bb-f01b-26f6643ff7d1"
   },
   "outputs": [],
   "source": [
    "homonym1 = 'За время обучения я прослушал больше сорока курсов.'\n",
    "homonym2 = 'Сорока своровала блестящее украшение со стола.'\n",
    "\n",
    "# корректно определелил части речи\n",
    "# NUM - числительное\n",
    "# S — существительное\n",
    "print(mystem_analyzer.analyze(homonym1)[-5])\n",
    "print(mystem_analyzer.analyze(homonym2)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4ifiT5avTY4"
   },
   "source": [
    "## Стемминг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3tkqE7jvTY4"
   },
   "source": [
    "В отличие от лемматизации, при применении стемминга у всех слов отбрасываются аффиксы (окончания и суффиксы), что необязательно приводит слова к формам, существующим в рассматриваемом языке.\n",
    "\n",
    "[**Snowball**](http://snowball.tartarus.org/) – фрэймворк для написания алгоритмов стемминга. Алгоритмы стемминга отличаются для разных языков и используют знания о конкретном языке – списки окончаний для разных чистей речи, разных склонений и т.д. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUI4LkoUvTY4"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_example = word_tokenize(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yDhsgmNsvTY5",
    "outputId": "9b29e2a4-1a9e-4e10-f247-3abdb81b14d6"
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('russian')\n",
    "stemmed_example = [stemmer.stem(w) for w in tokenized_example]\n",
    "print(' '.join(stemmed_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для английского получится что-то такое."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IYdj8R20vTY5",
    "outputId": "1bd95bdf-b5f2-42c2-a479-acdf03c58809"
   },
   "outputs": [],
   "source": [
    "text = \"In my younger and more vulnerable years my father gave me some advice that I've been turning over in my mind ever since.\\n\\\"Whenever you feel like criticizing any one,\\\" he told me, \\\"just remember that all the people in this world haven't had the advantages that you've had.\\\"\"\n",
    "print(text)\n",
    "text_tokenized = [w for w in word_tokenize(text) if w.isalpha()]\n",
    "print('==========')\n",
    "print(text_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "386kIQJYvTY5",
    "outputId": "3e3efc61-d8ec-4af6-d907-598ab3acb7f0"
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "text_stemmed = [stemmer.stem(w) for w in text_tokenized]\n",
    "print(' '.join(text_stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IS3FTOIXvTY6"
   },
   "source": [
    "_Что почитать:_\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Stemming\n",
    "- https://en.wikipedia.org/wiki/Lemmatisation\n",
    "- https://www.datacamp.com/community/tutorials/stemming-lemmatization-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ks-scoLGvTY6"
   },
   "source": [
    "## Bag-of-words и TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5dNFUPHvTY6"
   },
   "source": [
    "Но как же все-таки работать с текстами, используя стандартные методы машинного обучения? Нужна выборка!\n",
    "\n",
    "\n",
    "### Bag-of-words\n",
    "\n",
    "Самый очевидный способ формирования признакового описания текстов — векторизация. Пусть у нас имеется коллекция текстов $D = \\{d_i\\}_{i=1}^l$ и словарь всех слов, встречающихся в выборке $V = \\{v_j\\}_{j=1}^d.$ В этом случае некоторый текст $d_i$ описывается вектором $(x_{ij})_{j=1}^d,$ где\n",
    "$$x_{ij} = \\sum_{v \\in d_i} [v = v_j].$$\n",
    "\n",
    "Таким образом, текст $d_i$ описывается вектором количества вхождений каждого слова из словаря в данный текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2eF8nZLvTY7"
   },
   "outputs": [],
   "source": [
    "texts = ['I like my cat.', 'My cat is the most perfect cat.', 'is this cat or is this bread?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IjG4gU5JvTY7",
    "outputId": "0d8650c5-7419-4689-ca67-c7729734991f"
   },
   "outputs": [],
   "source": [
    "texts_tokenized = [' '.join([w for w in word_tokenize(t) if w.isalpha()]) for t in texts]\n",
    "texts_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVomQglIvTY8"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cnt_vec = CountVectorizer()\n",
    "X = cnt_vec.fit_transform(texts_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kwAKWt_4vTY9",
    "outputId": "1c8f8dad-89a9-4402-adae-916f76ae1631"
   },
   "outputs": [],
   "source": [
    "cnt_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ye63VKazvTY-",
    "outputId": "4dede0ed-ac68-4bad-ef3e-bb5be72f4dbc"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PvYOzpALvTY-",
    "outputId": "0b3192e7-2546-4040-cac8-55b208b30071"
   },
   "outputs": [],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "z40jdf5-vTY-",
    "outputId": "80b68939-a2b0-4ea1-909e-409e9b1b6310"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X.toarray(), columns=cnt_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tZsL30hvTY-"
   },
   "source": [
    "### TF-IDF\n",
    "\n",
    "Заметим, что если слово часто встречается в одном тексте, но почти не встречается в других, то оно получает для данного текста большой вес, ровно так же, как и слова, которые часто встречаются в каждом тексте. Для того, чтобы разделять эти такие слова, можно использовать статистическую меру TF-IDF, характеризующую важность слова для конкретного текста. Для каждого слова из текста $d$ рассчитаем относительную частоту встречаемости в нем (Term Frequency):\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{C(t | d)}{\\sum\\limits_{k \\in d}C(k | d)},\n",
    "$$\n",
    "где $C(t | d)$ - число вхождений слова $t$ в текст $d$.\n",
    "\n",
    "Также для каждого слова из текста $d$ рассчитаем обратную частоту встречаемости в корпусе текстов $D$ (Inverse Document Frequency):\n",
    "$$\n",
    "\\text{IDF}(t, D) = \\log\\left(\\frac{|D|}{|\\{d_i \\in D \\mid t \\in d_i\\}|}\\right)\n",
    "$$\n",
    "Логарифмирование здесь проводится с целью уменьшить масштаб весов, ибо зачастую в корпусах присутствует очень много текстов.\n",
    "\n",
    "В итоге каждому слову $t$ из текста $d$ теперь можно присвоить вес\n",
    "$$\n",
    "\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\n",
    "$$\n",
    "Интерпретировать формулу выше несложно: действительно, чем чаще данное слово встречается в данном тексте и чем реже в остальных, тем важнее оно для этого текста.\n",
    "\n",
    "Отметим, что в качестве TF и IDF можно использовать другие [определения](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Definition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0YcEWeqvTY_"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X = tfidf_vec.fit_transform(texts_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fpCVQJG8vTY_",
    "outputId": "f8290c90-e00b-4cef-cff8-538702ff4ba7"
   },
   "outputs": [],
   "source": [
    "tfidf_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "YGpSpl4KvTZA",
    "outputId": "266e8fdd-a1ae-4c1c-a4b8-2a950abd0e03"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X.toarray(), columns=tfidf_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iwcgx05UvTZA"
   },
   "source": [
    "**Вопросик:** Что изменилось по сравнению с использованием метода `CountVectorizer`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVDDSrJNvTZB"
   },
   "source": [
    "_Что почитать:_\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "- https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrP93rWwvTZB"
   },
   "source": [
    "## Baseline: классификация необработанных n-грамм\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mMoCnsi5vTZB"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression # можно заменить на любимый классификатор\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUAoWQ1-vTZC"
   },
   "source": [
    "Что такое n-граммы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KeFXNVbPvTZC"
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lIf_da6RvTZD",
    "outputId": "9e811e97-bc9b-4bc4-a980-b164d1a5a575"
   },
   "outputs": [],
   "source": [
    "sent = 'Если б мне платили каждый раз'.split()\n",
    "list(ngrams(sent, 1)) # униграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YhaYTVyuvTZD",
    "outputId": "dbd0ae5c-d5eb-4e1f-e501-7e54476014b7"
   },
   "outputs": [],
   "source": [
    "list(ngrams(sent, 2)) # биграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3fpat_ZGvTZD",
    "outputId": "a49249bd-6199-41ff-85cd-fe108b2a684a"
   },
   "outputs": [],
   "source": [
    "list(ngrams(sent, 3)) # триграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wK2QdeycvTZE",
    "outputId": "c8dc6ac6-1ee8-4cc1-8526-47cd8dbfaa40"
   },
   "outputs": [],
   "source": [
    "list(ngrams(sent, 5)) # ... пентаграммы?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1OpE-oFvTZE"
   },
   "source": [
    "## Задача: классификация твитов по тональности\n",
    "\n",
    "У нас есть датасет из твитов, про каждый указано, как он эмоционально окрашен: положительно или отрицательно. Задача: предсказывать эмоциональную окраску.\n",
    "\n",
    "Классификацию по тональности используют, например, в рекомендательных системах, чтобы понять, понравилось ли людям кафе, кино, etc.\n",
    "\n",
    "Скачиваем куски датасета ([источник](http://study.mokoron.com/)): [положительные](https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=0), [отрицательные](https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wNmExWmlvTZF",
    "outputId": "148f293b-12b9-479f-e801-32a62786b5f2"
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
    "!wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "km4cEWTavTZF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LLrigaDsvTZG"
   },
   "outputs": [],
   "source": [
    "# считываем данные и заполняем общий датасет\n",
    "positive = pd.read_csv('positive.csv', sep=';', usecols=[3], names=['text'])\n",
    "positive['label'] = 'positive'\n",
    "negative = pd.read_csv('negative.csv', sep=';', usecols=[3], names=['text'])\n",
    "negative['label'] = 'negative'\n",
    "df = positive.append(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIvPNAM5vTZH",
    "outputId": "5363dec1-d738-4d40-910d-c7ffba3e86de"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "stemmer_ru = nltk.stem.snowball.RussianStemmer()\n",
    "\n",
    "def stem_text(text, stemmer):\n",
    "    tokens = text.split()\n",
    "    stemmed_tokens = [stemmer.stem(w) for w in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "stemmed_texts = []\n",
    "for t in tqdm(df.text[1:10]):\n",
    "    stemmed_texts.append(stem_text(t, stemmer_ru))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5iQv8gWdvTZH"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df.text, df.label, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwtsmgibvTZI"
   },
   "source": [
    "Как уже обсуждали, самый простой способ извлечь признаки из текстовых данных — векторизаторы: `CountVectorizer` и `TfidfVectorizer`\n",
    "\n",
    "Объект `CountVectorizer` делает простую вещь:\n",
    "* строит для каждого документа (каждой пришедшей ему строки) вектор размерности `n`, где `n` -- количество слов или n-грам во всём корпусе\n",
    "* заполняет каждый i-тый элемент количеством вхождений слова в данный документ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_count_vec = make_pipeline(CountVectorizer(ngram_range=(1, 1)),\n",
    "                                MaxAbsScaler(), \n",
    "                              LogisticRegression(max_iter=200, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E64CpRsjvTZJ"
   },
   "source": [
    "ngram_range отвечает за то, какие n-граммы мы используем в качестве фичей:<br/>\n",
    "ngram_range=(1, 1) — униграммы<br/>\n",
    "ngram_range=(3, 3) — триграммы<br/>\n",
    "ngram_range=(1, 3) — униграммы, биграммы и триграммы.\n",
    "\n",
    "В `vec.vocabulary_` лежит словарь: мэппинг слов к их индексам:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_count_vec.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pipe_count_vec.predict(x_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipe_count_vec['countvectorizer'].vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3a56F0vivTZL"
   },
   "source": [
    "Попробуем сделать то же самое для триграмм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_count_vec_3 = make_pipeline(CountVectorizer(ngram_range=(1, 1)),\n",
    "                                MaxAbsScaler(), \n",
    "                              LogisticRegression(max_iter=200, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_count_vec_3.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ytgHKnjvTZJ",
    "outputId": "623848b0-b7d4-42b2-a985-59c14f92a8d8"
   },
   "outputs": [],
   "source": [
    "pred_3 = pipe_count_vec_3.predict(x_test)\n",
    "print(classification_report(y_test, pred_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyW9lA_6vTZL"
   },
   "source": [
    "А теперь для TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_count_vec = make_pipeline(TfidfVectorizer(ngram_range=(1, 1)),\n",
    "                              LogisticRegression(max_iter=400, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4jqKUFgvTZL"
   },
   "outputs": [],
   "source": [
    "pipe_count_vec.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tfidf = pipe_count_vec.predict(x_test)\n",
    "print(classification_report(y_test, pred_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ixapx51FvTZM"
   },
   "source": [
    "## О важности эксплоративного анализа\n",
    "\n",
    "Но иногда пунктуация бывает и не шумом — главное отталкиваться от задачи. Что будет если вообще не убирать пунктуацию?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_punct = make_pipeline(CountVectorizer(ngram_range=(1, 1),  tokenizer=word_tokenize),\n",
    "                                MaxAbsScaler(), \n",
    "                              LogisticRegression(max_iter=200, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSxY-8aRvTZM"
   },
   "outputs": [],
   "source": [
    "pipe_punct.fit(x_train, y_train)\n",
    "pred_punct = pipe_punct.predict(x_test)\n",
    "print(classification_report(y_test, pred_punct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_8uz4TevTZM"
   },
   "source": [
    "Стоило оставить пунктуацию — и внезапно все метрики устремились к 1. Как это получилось? Среди неё были очень значимые токены (как вы думаете, какие?). Найдите признаки с самыми большими коэффициентами:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZC8Mb6bvTZM"
   },
   "source": [
    "Посмотрим, как один из супер-значительных токенов справится с классификацией безо всякого машинного обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vvWdQbYMvTZN"
   },
   "outputs": [],
   "source": [
    "cool_token = \n",
    "pred = ['positive' if cool_token in tweet else 'negative' for tweet in x_test]\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3nD71B-ZvTZO"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "tweets_with_cool_token = [tweet for tweet in x_train if cool_token in tweet]\n",
    "for tweet in np.random.choice(tweets_with_cool_token, size=10, replace=False):\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dv6P8ncIvTZO"
   },
   "source": [
    "## Символьные n-граммы\n",
    "\n",
    "Теперь в качестве признаков используем, например, униграммы символов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_symbols = make_pipeline(CountVectorizer(ngram_range=(1, 1), analyzer='char'),\n",
    "                                MaxAbsScaler(), \n",
    "                              LogisticRegression(max_iter=200, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipe_symbols['countvectorizer'].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cATsmweivTZO"
   },
   "outputs": [],
   "source": [
    "pipe_symbols.fit(x_train, y_train)\n",
    "pred_symbols = pipe_punct.predict(x_test)\n",
    "print(classification_report(y_test, pred_symbols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PK8-_xwvTZO"
   },
   "source": [
    "В общем-то, теперь уже понятно, почему на этих данных здесь 1. Так или иначе, на символах классифицировать тоже можно: для некоторых задач (например, для определения языка) признаки-символьные n-граммы могут внести серьезный вклад в качество модели.\n",
    "\n",
    "Ещё одна замечательная особенность признаков-символов: токенизация и лемматизация не нужна, можно использовать такой подход для языков, у которых нет готовых анализаторов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoXhAroLvTZP"
   },
   "source": [
    "_Что почитать:_\n",
    "\n",
    "- https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "- https://books.google.com/ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "islq37BzvTZP"
   },
   "source": [
    "## Регулярные выражения\n",
    "\n",
    "https://ru.wikipedia.org/wiki/Регулярные_выражения\n",
    "\n",
    "Вообще, часто бывает так, что для конкретного случая нужен особый способ токенизации, и надо самостоятельно написать регулярное выражение. Или, например, перед работой с текстом, надо почистить его от своеобразного мусора: упоминаний пользователей, url и так далее.\n",
    "\n",
    "Навык полезный, давайте в нём тоже потренируемся."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymGN9r6gvTZP"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qJxZd2pvTZQ"
   },
   "source": [
    "### findall\n",
    "возвращает список всех найденных совпадений\n",
    "\n",
    "- ? : ноль или одно повторение\n",
    "- \\* : ноль или более повторений\n",
    "- \\+ : одно или более повторений\n",
    "- . : любой символ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hn6AnrvtvTZQ"
   },
   "outputs": [],
   "source": [
    "result = re.findall('ab+c.', 'abcdefghijkabcabcxabc') \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHOTT9XTvTZQ"
   },
   "source": [
    "Вопрос на внимательность: почему нет abcx?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k7gzrPCOvTZR"
   },
   "outputs": [],
   "source": [
    "re.findall('ab+c.', 'abbbca')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qxi1znlvTZR"
   },
   "source": [
    "### split\n",
    "разделяет строку по заданному шаблону\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3Qz-3RSvTZR"
   },
   "outputs": [],
   "source": [
    "result = re.split(',', 'itsy, bitsy, teenie, weenie') \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Ur6WK3GvTZR"
   },
   "source": [
    "можно указать максимальное количество разбиений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pwu8L0LevTZS"
   },
   "outputs": [],
   "source": [
    "result = re.split(',', 'itsy, bitsy, teenie, weenie', maxsplit=2) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGFk3MpOvTZS"
   },
   "source": [
    "### sub\n",
    "ищет шаблон в строке и заменяет все совпадения на указанную подстроку\n",
    "\n",
    "параметры: (pattern, repl, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azeAmGOBvTZS"
   },
   "outputs": [],
   "source": [
    "result = re.sub('a', 'b', 'abcabc')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HY8kwgOVvTZS"
   },
   "source": [
    "### compile\n",
    "компилирует регулярное выражение в отдельный объект"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nudrx7nJvTZS"
   },
   "outputs": [],
   "source": [
    "# Пример: построение списка всех слов строки:\n",
    "prog = re.compile('[А-Яа-яё\\-]+')\n",
    "prog.findall(\"Слова? Да, больше, ещё больше слов! Что-то ещё.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YONOWD3XvTZS"
   },
   "source": [
    "**Задание**: вернуть список доменов (@gmail.com) из списка адресов электронной почты:\n",
    "\n",
    "```\n",
    "abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLWpguIsvTZT"
   },
   "outputs": [],
   "source": [
    "emails = 'abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayTzy2lCvTZT"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoHLF2IQvTZT"
   },
   "source": [
    "_Что почитать:_\n",
    "\n",
    "- https://habr.com/ru/post/115825/\n",
    "- https://www.regular-expressions.info/\n",
    "- https://regexr.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JUST FOR FUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge wordcloud -y\n",
    "import wordcloud as wc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(subset='all', categories=['sci.med'], remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.array(Image.open(\"sherlock.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_im = wc.WordCloud(width = 3000, \n",
    "                     height = 2000, \n",
    "                     random_state=1, \n",
    "                     background_color='white', \n",
    "                     contour_color='steelblue',\n",
    "                     contour_width=3,\n",
    "                     collocations=False,\n",
    "                     mask=mask,\n",
    "                     stopwords = stopwords.words('english')).generate(data['data'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 12))\n",
    "plt.imshow(wc_im) \n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of sem10_texts.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
