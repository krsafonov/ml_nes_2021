{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашняя работа 4. Деревья\n",
    "\n",
    "*Очень мягкий дедлайн: 26 ноября, 21:00*\n",
    "\n",
    "*Очень жесткий дедлайн: 29 ноября, 21:00*\n",
    "\n",
    "\n",
    "### Оценивание и штрафы\n",
    "\n",
    "Максимальная оценка — 10 баллов и 1 бонус.\n",
    "\n",
    "Есть опция забить на задание 2, взять готовый класс из `sklearn` и решить три последних задания с ним. Тогда максимальная оценка за домашку — 7 из 10 :)\n",
    "\n",
    "Не списывайте, иначе всем участникам обнулим :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Задание 1 (2 балла)\n",
    "Реализуйте функцию `find_best_split`, которая должна находить оптимальное разбиение подмножества обучающей выборки. Мы уже делали что-то подобное на семинаре: теперь немного усложните логику функции, в частности добавьте возможность выбора критерия информативности и типа признака. Протестируйте эту функцию на каком-нибудь датасете с бинарным таргетом. Можно взять готовый из `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используйте следующий функционал ошибки:\n",
    "\n",
    "$$Q(R) = -\\frac {|R_{\\ell}|}{|R|}H(R_\\ell) -\\frac {|R_r|}{|R|}H(R_r) ,$$\n",
    "\n",
    "где $R$ — множество объектов, попавших в вершину, $R_{\\ell}$ и $R_r$ — объекты, попавшие в левое и правое поддеревья,\n",
    "$H(R)$ — критерий информативности (Джини или энтропия).\n",
    "\n",
    "\n",
    "В этой реализации для категориальных признаков применяется наивный алгоритм разбиения: мы пытаемся найти одно значение, разбиение по которому сильнее всего улучшит функционал качества на данном шаге. Иными словами, объекты с конкретным значением признака отправляем в левое поддерево, остальные - в правое. Обратите внимание, что это далеко не оптимальные способ учёта категориальных признаков. Например, можно было бы на каждое значение категориального признака создавать отдельное поддерево или использовать более сложные подходы. Подробнее об этом можно прочитать в конспектах [лекций](https://github.com/esokolov/ml-course-hse/blob/master/2019-fall/lecture-notes/lecture07-trees.pdf) по машинному обучению на ПМИ (раздел «Учёт категориальных признаков»).\n",
    "\n",
    "Если вы захотите решить более сложный вариант домашки с учетом категориальных признаков, что сильно больше походит на реальность, напишите мне, и я выдам вам другой вариант домашки :) Он будет несколько сложнее, но интересней. Бонусов за это не будет :)\n",
    "\n",
    "**Note:** постарайтесь решить без использования циклов для перебора порогов — впрочем, снимать баллы именно за этот момент не будем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(\n",
    "    feature_vector: Union[np.ndarray, pd.DataFrame], \n",
    "    target_vector: Union[np.ndarray, pd.Series],\n",
    "    criterion: str = \"gini\",\n",
    "    feature_type: str = \"real\"\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, float, float]:\n",
    "    \"\"\"\n",
    "    Указания:\n",
    "    * Пороги, приводящие к попаданию в одно из поддеревьев пустого множества объектов, не рассматриваются.\n",
    "    * В качестве порогов, нужно брать среднее двух сосдених (при сортировке) значений признака\n",
    "    * Поведение функции в случае константного признака может быть любым.\n",
    "    * При одинаковых приростах Джини или энтропии нужно выбирать порог с наименьшим значением.\n",
    "\n",
    "    :param feature_vector: вещественнозначный вектор значений признака\n",
    "    :param target_vector: вектор классов объектов,  len(feature_vector) == len(target_vector)\n",
    "    :param criterion: либо `gini`, либо `entropy`\n",
    "    :param feature_type: либо `real`, либо `categorical`\n",
    "    \n",
    "    :return thresholds: отсортированный по возрастанию вектор со всеми возможными порогами, по которым объекты можно\n",
    "     разделить на две различные подвыборки\n",
    "    :return q_values: вектор со значениями функционала ошибки для каждого из порогов в thresholds len(q_values) == len(thresholds)\n",
    "    :return threshold_best: оптимальный порог\n",
    "    :return q_value_best: значение функционала ошибки при оптимальном разбиении\n",
    "    \"\"\"\n",
    "    # ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————-\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведите график зависимости значения критерия ошибки от порогового значения при разбиении по любому признаку из выбранного вами датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2 (3 балла)\n",
    "Разберитесь с написанным кодом решающего дерева, заполните пропуски в коде и реализуйте недостающий метод _predict_node.\n",
    "\n",
    "Построение дерева осуществите согласно базовому жадному алгоритму.\n",
    "\n",
    "- Выбор лучшего разбиения необходимо производить по указанному критерию информативности.\n",
    "- Критерий останова: все объекты в листе относятся к одному классу или ни по одному признаку нельзя разбить выборку.\n",
    "- Ответ в листе: наиболее часто встречающийся класс.\n",
    "- **1 бонусный балл** можно получить за поддержку параметров `max_depth`, `min_samples_leaf` и `min_samples_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(\n",
    "        self, \n",
    "        feature_types: Union[List[str], np.ndarray], \n",
    "        max_depth: int = None, \n",
    "        min_samples_split: int = None, \n",
    "        min_samples_leaf: int = None,\n",
    "        criterion: str = 'gini'\n",
    "    ) -> None:\n",
    "        \n",
    "        if np.any(list(map(lambda x: x != \"real\" and x != \"categorical\", feature_types))):\n",
    "            raise ValueError(\"There is unknown feature type\")\n",
    "\n",
    "        # В этой переменной будем хранить узлы решающего дерева. Каждая вершина хранит в себе идентификатор того,\n",
    "        # является ли она листовой. Листовые вершины хранят значение класса для предсказания, нелистовые - правого и\n",
    "        # левого детей (поддеревья для продолжения процедуры предсказания)\n",
    "        self._tree = {}\n",
    "        \n",
    "        # типы признаков (категориальные или числовые)\n",
    "        self._feature_types = feature_types\n",
    "        \n",
    "        # гиперпараметры дерева\n",
    "        self._max_depth = max_depth\n",
    "        self._min_samples_split = min_samples_split\n",
    "        self._min_samples_leaf = min_samples_leaf\n",
    "        self._criterion = criterion\n",
    "\n",
    "    def _fit_node(\n",
    "        self, \n",
    "        sub_X: np.ndarray, \n",
    "        sub_y: np.ndarray, \n",
    "        node: dict\n",
    "    ) -> None:\n",
    "        \n",
    "        # критерий останова\n",
    "        if np.all(sub_y == sub_y[0]):\n",
    "            node[\"type\"] = \"terminal\"\n",
    "            node[\"class\"] = sub_y[0]\n",
    "            return\n",
    "\n",
    "        feature_best, threshold_best, q_value_best, split = None, None, None, None\n",
    "        for feature in range(sub_X.shape[1]):\n",
    "            feature_type = self._feature_types[feature]\n",
    "\n",
    "            # подготавливаем признак для поиска оптимального порога\n",
    "            if feature_type == \"real\":\n",
    "                feature_vector = sub_X[:, feature]\n",
    "            elif feature_type == \"categorical\":\n",
    "                # здесь могла быть реализация более сложного подхода к обработке категориального признака\n",
    "                feature_vector = sub_X[:, feature]\n",
    "\n",
    "            # ищем оптимальный порог\n",
    "            _, _, threshold, q_value = find_best_split(feature_vector, sub_y, self._criterion, feature_type)\n",
    "            \n",
    "            if q_value_best is None or q_value > q_value_best:\n",
    "                feature_best = feature\n",
    "                q_value_best = q_value\n",
    "\n",
    "                # split - маска на объекты, которые должны попасть в левое поддерево\n",
    "                if feature_type == \"real\":\n",
    "                    threshold_best = threshold\n",
    "                    split = # ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————-\n",
    "                elif feature_type == \"categorical\":\n",
    "                    # в данной реализации это просто значение категории\n",
    "                    threshold_best = threshold\n",
    "                    split = # ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————-\n",
    "                else:\n",
    "                    raise ValueError\n",
    "\n",
    "        # записываем полученные сплиты в атрибуты класса\n",
    "        if feature_best is None:\n",
    "            node[\"type\"] = \"terminal\"\n",
    "            node[\"class\"] = Counter(sub_y).most_common(1)[0][0]\n",
    "            return\n",
    "\n",
    "        node[\"type\"] = \"nonterminal\"\n",
    "\n",
    "        node[\"feature_split\"] = feature_best\n",
    "        if self._feature_types[feature_best] == \"real\":\n",
    "            node[\"threshold\"] = threshold_best\n",
    "        elif self._feature_types[feature_best] == \"categorical\":\n",
    "            node[\"category_split\"] = threshold_best\n",
    "        else:\n",
    "            raise ValueError\n",
    "            \n",
    "        node[\"left_child\"], node[\"right_child\"] = {}, {}\n",
    "        self._fit_node(sub_X[split], sub_y[split], node[\"left_child\"])\n",
    "        self._fit_node(sub_X[np.logical_not(split)], sub_y[np.logical_not(split)], node[\"right_child\"])\n",
    "\n",
    "    def _predict_node(self, x: np.ndarray, node: dict) -> int:\n",
    "        \"\"\"\n",
    "        Предсказание начинается с корневой вершины дерева и рекурсивно идёт в левое или правое поддерево\n",
    "        в зависимости от значения предиката на объекте. Листовая вершина возвращает предсказание.\n",
    "        :param x: np.array, элемент выборки\n",
    "        :param node: dict, вершина дерева\n",
    "        \"\"\"\n",
    "        # ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————-\n",
    "        pass\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> DecisionTree:\n",
    "        self._fit_node(X, y, self._tree)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        predicted = []\n",
    "        for x in X:\n",
    "            predicted.append(self._predict_node(x, self._tree))\n",
    "            \n",
    "        return np.array(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3 (2 балла)\n",
    "Загрузите таблицу `students.csv` (это немного преобразованный датасет [User Knowledge](https://archive.ics.uci.edu/ml/datasets/User+Knowledge+Modeling)). В ней признаки объекта записаны в первых пяти столбцах, а в последнем записана целевая переменная (класс: 0 или 1). Постройте на одном изображении пять кривых \"порог — значение критерия Джини\" для всех пяти признаков. Отдельно визуализируйте scatter-графики \"значение признака — класс\" для всех пяти признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4 (1 балл)\n",
    "\n",
    "Исходя из кривых значений критерия Джини, по какому признаку нужно производить деление выборки на два поддерева? Согласуется ли этот результат с визуальной оценкой scatter-графиков? Как бы охарактеризовали вид кривой для \"хороших\" признаков, по которым выборка делится почти идеально? Чем отличаются кривые для признаков, по которым деление практически невозможно?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5 (2 балла)\n",
    "\n",
    "Протестируйте свое решающее дерево на датасете [mushrooms](https://archive.ics.uci.edu/ml/datasets/Mushroom). Вам нужно скачать таблицу agaricus-lepiota.data (из [Data Folder](https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/)). Обучите решающее дерево на половине случайно выбранных объектов (признаки в датасете категориальные) и сделайте предсказания для оставшейся половины. Посчитайте метрики качества."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————-"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
